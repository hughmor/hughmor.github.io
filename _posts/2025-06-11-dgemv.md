---
layout: default
title: "so how long does matrix-vector-multiplication take?"
description: "a timing experiment of matvec on cpu, gpu & apple silicon"
---

## intro

I've been in the world of analog photonics for 6 years now, and every time the need for comparisons against digital architectures comes up, I find myself sucked deeper and deeper into the rabbit hole of the digital world. Recently it became clear that---despite my classes in computer architecture---I actually didn't know all that much about the shape of the digital computing landscape before getting into a field where we're proposing alternative models of processing. Recently in the lab, I've been messing around a bit more with some RFSoC FPGAs I want to use for testing my chips, at the same time I've been working on the revisions for a manuscript where a reviewer asked some pretty tough questions about the ultimate performance of a photonic processor compared to SIMD architectures. This led to simultaneous deep dive into the world of FPGAs, and an analysis of how NVIDIA's GPUs actually work. This post is going to be the first entry in a series where I teach myself the digital side of the equation.

In the past week or so, I've started with running some tests to get an understanding of raw matrix computation latency. I want to understand what orders of magnitude can be spanned with different compute architectures and software libraries like CUDA, BLAS, and Apple's Accelerate and Metal Performance Shaders. This post focuses on double-precision matrix-vector multiplication (MVM), or---using the BLAS terminology---`dgemv` (Double-precision GEneral Matrix-Vector product). Before diving into each approach I looked at, take a look at the overall results:

![mvm results]({{ "assets/img/mvm-benchmarking/gemv_fp64_cpu_gpu_bench.svg" | relative_url }})


## naive mvm on single cpu core
My first step was to write a reference function that implements MVM in the most naive way possible. Since I'm interested in benchmarking my hardware, I want to run a C function so that I can do timing without the python overhead. I'm still going to write my benchmarking script in python for now, but I can launch this with `ctypes`.

```
// cpu_ref.c
#include <stddef.h>

void dgemv_ref(const double *A, const double *x,
               double *y, size_t m, size_t n)
{
    for (size_t i = 0; i < m; ++i) {
        double acc = 0.0;
        for (size_t j = 0; j < n; ++j)
            acc += A[i*n + j] * x[j];
        y[i] = acc;
    }
}
```

Since I know python better, I still wrote my launcher script using python's `ctypes` library.


## blas mvm using apple's accelerate library
In order to optimize this further, we can leverage the open library of basic linear algebra subroutines (BLAS). Apple has an API called [Accelerate](https://developer.apple.com/documentation/accelerate), which implements the optimized BLAS routines for their hardware.

I wrote a small C function to wrap the double-precision general matrix-vector product function `cblas_dgemv` so that I can call it from my Python benchmarking script.

```
// cpu_blas.c
#define ACCELERATE_NEW_LAPACK 1
#include <Accelerate/Accelerate.h>

void dgemv_blas(const double *A, const double *x,
                double *y, size_t m, size_t n)
{
    const double alpha = 1.0, beta  = 0.0;

    int M = (int)m;
    int N = (int)n;
    int lda = N;

    cblas_dgemv(CblasRowMajor, CblasNoTrans,
                M, N,
                alpha, A, lda,
                x, 1,
                beta,  y, 1);
}
```



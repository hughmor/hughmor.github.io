---
layout: default
title: "so how long does matrix-vector-multiplication take?"
description: "a timing experiment of gemv on cpu, gpu & apple silicon"
---

I've been working in analog photonics for about 6 years now, and every time the need for comparisons against digital architectures comes up, I find myself sucked deeper and deeper into the rabbit hole of the digital world. Recently it became clear that I didn't actually know all that much about the shape of the digital computing landscape before getting into a field where we're proposing alternative models of processing---despite the computing specialization in my Applied Physics degree (I'll save my rant on the state of engineering education for another day...). Recently, though, in the lab, I've been messing around with some RFSoC FPGA dev boards that I want to use for testing my chips, at the same time as I've been working on appeasing a tough reviewer who asked some hard questions about the ultimate performance of a photonic processor compared to SIMD in the revision process of our latest manuscript. This led me to a simultaneous investigation into FPGAs and an analysis of NVIDIA's GPU architecture. This post is going to be the first entry in a series of software and hardware experiments that I'm doing to try to learn more.

## overview
In the past week or so, I've started with running some tests to get an understanding of raw matrix computation latency. I want to see what orders of magnitude can be spanned with different compute architectures and software libraries like CUDA, BLAS, and Apple's Accelerate and Metal Performance Shaders. This post focuses on double-precision matrix-vector multiplication (MVM), or `dgemv` using the BLAS terminology (Double-precision GEneral Matrix-Vector product). Before diving into each approach, let's take a look at the overall results for latency as a function of matrix/vector size ($N$):

![mvm results]({{ "assets/img/mvm-benchmarking/gemv_fp64_cpu_gpu_bench_v2.svg" | relative_url }})


It's pretty nice to see right off the bat that every approach follows an $\mathcal{O}(N^2)$ asymptotic limit. On the other hand, we can also see some fairly significant differences between approaches. I mean just look at how terrible python is ðŸ˜…. Even ignoring the obvious, there's almost 3 orders of magnitude between the worst-case unoptimized MVM on a single-core CPU and the best result I got using an A6000 GPU!

## methodology
One note of caution I'll make right off the bat is that I wrote my testbench in python, so all the timing numbers should be taken with some grain of salt. I did do a very quick control experiment on this front that I'll explain later, and it seems like Python's overhead only affects things at the low scale end (see the red curve compared to orange).

For each hardware approach that I want to compare, I wrote a small `dgemv` function that computes the matrix-vector product, and then launch data collection from python. I execute 10,000 trials using the same data for each approach, and collect the timing deltas. 

```
METHODS = {
    ... # mapping from label->function
}

def run_benchmarking_test(m, n, repeats=10_000, skip=False):
    rng = np.random.default_rng(0)
    A = rng.standard_normal((m, n), dtype=np.float64)
    x = rng.standard_normal(n, dtype=np.float64)
    y = np.empty(m, dtype=np.float64)

    for method, fn in METHODS.items():
        ... # set-up function arguments, saving, etc.

        bench(method, fn, *args) # warmâ€‘up hardware
        deltas = np.zeros(repeats, dtype=np.float64)
        for i in range(repeats):
            deltas[i] = bench(method, fn, *args)

```

All my timing data is collected using `time.perf_counter` in the `bench` function.
```
def bench(label, fn, *args):
    t0 = time.perf_counter()
    fn(*args)
    t1 = time.perf_counter()
    return t1 - t0
```

Now, I'll get into some more implementation details about each benchmark function I wrote, and some remarks about the results seen in each one.

## naive mvm on single cpu core
The first thing I wanted to do was write MVM in the most naive way possible.

```
// cpu_ref.c
#include <stddef.h>

void dgemv_ref(const double *A, const double *x,
               double *y, size_t m, size_t n)
{
    for (size_t i = 0; i < m; ++i) {
        double acc = 0.0;
        for (size_t j = 0; j < n; ++j)
            acc += A[i*n + j] * x[j];
        y[i] = acc;
    }
}
```

Since I know python better, I still wrote my launcher script using python's `ctypes` library.


## blas mvm using apple's accelerate library
In order to optimize this further, we can leverage the open library of basic linear algebra subroutines (BLAS). Apple has an API called [Accelerate](https://developer.apple.com/documentation/accelerate), which implements the optimized BLAS routines for their hardware.

I wrote a small C function to wrap the double-precision general matrix-vector product function `cblas_dgemv` so that I can call it from my Python benchmarking script.

```
// cpu_blas.c
#define ACCELERATE_NEW_LAPACK 1
#include <Accelerate/Accelerate.h>

void dgemv_blas(const double *A, const double *x,
                double *y, size_t m, size_t n)
{
    const double alpha = 1.0, beta  = 0.0;

    int M = (int)m;
    int N = (int)n;
    int lda = N;

    cblas_dgemv(CblasRowMajor, CblasNoTrans,
                M, N,
                alpha, A, lda,
                x, 1,
                beta,  y, 1);
}
```


## next steps
Like I said in the intro, I've also been learning about FPGAs recently and looking at different ways I could do `dgemv` on the FPGA hardware sounds like a great project to help accelerate my progress on that front, so I'm going to look into that next. There's a lot of surface area there that I don't even totally understand yet, but my ultimate goal is going to be building a systolic array matrix multiplier in the FPGA fabric, and I'll add it to the comparison plots.

I should also look into how the precision of the operands factors in here. I know NVIDIA quotes different throughput specs depending on bit precision, but I'm not sure how CPUs will fare in comparison on that front. I think the peak throughput directly relates to bit precision, but does this hold for a real processor? Will half the bits get us half the latency in practice? I'll look into it!

On top of that, an obvious extension will be to see how matrix-matrix multiplication compares to MVM. My overall expectation is that the relative standings of each approach won't change too much, but of course the overall trend will change from $\mathcal{O}(N^2)$ to $\mathcal{O}(N^3)$. Maybe I'm wrong though! I could see it being plausible that the truly parallel hardware like GPUs (and the FPGA systolic array) will have a drastic speedup over the CPU in terms of absolute magnitude.